import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

#!/usr/bin/env python3
import argparse, json, re, os
from typing import List, Dict, Any

from data_pipeline.omni_retriever import OmniRetriever
from data_pipeline.filter import SafetyFilter
from knowledge_base.database import KnowledgeBase
from knowledge_base.chunk_store import ChunkStore
from learning_module.trainer_online import OnlineTrainer
from learning_module.embedding_bridge import EmbeddingBridge
from retrieval.chunk_index import ChunkIndex

CHUNK_SIZE = 900
CHUNK_OVERLAP = 140

def extract_keywords(text: str, n=5) -> str:
    words = re.findall(r"\b[a-zA-Z]{5,}\b", (text or "").lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    return ", ".join([w for w, _ in sorted(freq.items(), key=lambda x: -x[1])[:n]])

def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    if len(t) <= size:
        return [t]
    out = []
    step = max(1, size - overlap)
    for i in range(0, len(t), step):
        c = t[i:i+size].strip()
        if len(c) > 80:
            out.append(c)
        if i + size >= len(t):
            break
    return out

def to_enriched_items(docs: List[Dict[str, Any]], topic: str) -> List[Dict[str, Any]]:
    enriched = []
    for d in docs:
        text = (d.get("text") or "").strip()
        if not text:
            continue
        enriched.append({
            "text": text,
            "paper_title": d.get("title", f"Unknown ({topic})"),
            "concepts": extract_keywords(text),
            "source": d.get("source", "unknown"),
        })
    return enriched

def to_chunks(enriched_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    chunks = []
    for it in enriched_docs:
        parts = chunk_text(it.get("text", ""))
        for p in parts:
            chunks.append({
                "text": p,
                "paper_title": it.get("paper_title", "unknown"),
                "source": it.get("source", "unknown"),
                "meta_json": json.dumps({"concepts": it.get("concepts","")}),
            })
    return chunks

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("topic", type=str)
    ap.add_argument("--limit", type=int, default=40)
    ap.add_argument("--rebuild_index", action="store_true")
    args = ap.parse_args()

    omni = OmniRetriever()
    safety = SafetyFilter()
    kb = KnowledgeBase()
    chunk_store = ChunkStore()

    print(f"ğŸ” Retrieving docs for: {args.topic}")
    docs = omni.retrieve_universal(args.topic)[: args.limit]
    safe_docs = safety.filter(docs)
    print(f"âœ… Retrieved={len(docs)} safe={len(safe_docs)}")

    enriched = to_enriched_items(safe_docs, args.topic)
    if not enriched:
        print("âš ï¸ No enriched docs. Nothing to store.")
        return

    kb.store(enriched)
    chunks = to_chunks(enriched)
    res = chunk_store.upsert_chunks(chunks)
    print(f"ğŸ§© ChunkStore added={res['added']} skipped={res['skipped']}")
    print(f"ğŸ“¦ chunks_in_store={chunk_store.count()}  kb_count={kb.count()}")

    if args.rebuild_index:
        trainer = OnlineTrainer()
        bridge = EmbeddingBridge(trainer)
        idx = ChunkIndex(chunk_store=chunk_store, encoder=bridge, cache_dir="data", max_items=8000, chunk_batch=32)
        idx.rebuild()
        print(f"âœ… Index rebuilt. items={len(idx.items)} dim={idx.dim}")

if __name__ == "__main__":
    main()
